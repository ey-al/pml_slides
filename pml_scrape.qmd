---
title: "Webscraping in R"
author: "Eyal Hanfling & Raymond Wang"
subtitle: "MIT Political Methodology Lab (PML) Workshop" 
date: 02/24/2023
date-format: long
execute: 
  echo: true
#format: 
#  revealjs:
#    incremental: true
format:
  revealjs:
    incremental: true
    embed-resources: true
editor: visual
---

```{r, include=FALSE}
setwd("~/Dropbox (MIT)/pml_scraping")
```

## Introduction[^1]

[^1]: Some material in this presentation is adapted from MIT PoliSci Quant IV Slides and Adam Kaplan/Andy Halterman's PML workshop slides

Today we will cover:

-   What is scraping

-   What are you scraping (basics of HTML)

-   What to do with the stuff your scrape (Regex, extraction)

-   Worked Examples: Scraping *International Security* and an archived Pakistani government website

## What is scraping

-   Scraping is the automated collection of unstructured data on the web.

-   Scraping public websites, even against the terms of service, is probably legal. See [hiQ Labs, Inc. v. LinkedIn Corp (2019).](https://www.eff.org/deeplinks/2019/09/victory-ruling-hiq-v-linkedin-protects-scraping-public-data)

-   But circumventing technical restrictions (passwords, captchas) is probably illegal.

-   Something different from scraping: calling an API (like the Twitter API or City of Cambridge API) to download data

## What do with Scraped Data

-   extract objects using Regular Expressions (Regex)
-   Use Optical Character Recognition (OCR) to extract text from a PDF with tools like [Tesseract](https://cran.r-project.org/web/packages/tesseract/vignettes/intro.html) or [daiR](https://dair.info/index.html)

## Before we start:

Helps to keep the **stringr** and **rvest** references open while coding!

::: columns
::: {.column width="50%"}
![stringr.tidyverse.org](stringr_logo.png){fig-align="center"}
:::

::: {.column width="50%"}
![rvest.tidyverse.org](rvest_logo.png){fig-align="center"}
:::
:::

------------------------------------------------------------------------

## Intro to HTML and CSS {.scrollable}

-   HTML stands for "[H]{style="color: red;"}yper[T]{style="color: red;"}ext [M]{style="color: red;"}arkup [L]{style="color: red;"}anguage"
    -   Content is enclosed in tags. Ex: `<title>Page Title<title>`
    -   Tags can have attributes. Ex: `<h1 id='first'>A heading</h1>`
        -   `h1`-`h6` for headings
        -   `p` for paragraph
        -   `a` for links
-   CSS is a language that styles elements of a page
    -   Select `<p>` tags, color red, font size to 12px:
    -   `p {color: red; font-size: 12px;}`

## Look at the source of [polisci.mit.edu](polisci.mit.edu)

![](raymondsource.png){fig-align="center"}

```{html}
#| eval: false
<div class="text">
  <div class="namearea">
        <h2><a href="/people/grad-student">Grad Student</a></h2>
      </div><!-- divnamearea -->

                    <div class="phonearea">
          <p class="phone">
                      <span class="email"><a href="mailto:gradstudent@mit.edu">gradstudent@mit.edu</a></span>
                                        </p>
```

## Intro to rvest

-   Start with `read_html()`
    -   Put in a link, output is an xml document
-   Use `html_element()` or `html_elements()` to extract an element
    -   Use a CSS selector to define the element
    -   Ex: `html_element("h1")`
-   Use `html_text()` to extract the text of an element
-   Use `html_attr()` to extract the value of an attribute

## Mini Example: scraping email addresses

```{html}
#| eval: false
<div class="text">
  <div class="namearea">
        <h2><a href="/people/grad-student">Grad Student</a></h2>
      </div><!-- divnamearea -->

                    <div class="phonearea">
          <p class="phone">
                      <span class="email"><a href="mailto:gradstudent@mit.edu">gradstudent@mit.edu</a></span>
                                        </p>
```

```{r}
#| eval: false
#| code-line-numbers: "|2|4|6|8"
#Read the HTML File
read_html("https://polisci.mit.edu/people/graduate-students") %>%
  #Extract HTML element <span>
  html_elements('span') %>% 
  #Extract HTML element <a>
  html_elements('a') %>% 
  #Extract HTML attribute href
  html_attr("href")
```

## Intro to Regex {.scrollable}

-   Regex stands for [Reg]{style="color: red;"}ular [Ex]{style="color: red;"}pression

-   Tool to manipulate text data, called strings

-   Main package is `stringr` and the `grep` family from base R

```{r}
#loading all packages needed
library(tidyverse)
library(rvest)
library(stringr)
library(lubridate)
library(glue)
library(httr)

text <- c("Boston is so cold", "It is 25 degrees in Hong Kong")
#grep returns index 
grep('cold', text)
#grepl returns logical vector of same length
grepl("cold", text)
#str_subset returns all matches in full
str_subset(text, 'cold')
#which is equivalent to:
text %>% .[grep('cold',.)]
```

## Intro to Regex {.scrollable}

Regex is composed of 3 components

-   *literal characters*: matched by a single characters, i.e. a, b, c

-   *character classes*: matched by any number of characters. Use brackets to denote: \[azx\], \[A-Z\]

-   *modifiers*: operate on literal characters or character classes. Denoted by special characters, such as +,\*,?

-   We combine these components to match patterns in strings and manipulate them

-   Regex [checker](https://regex101.com/)!

```{r}
text <- c("Boston is so cold", "It is 25 degrees in Hong Kong",
          "02139")
text %>% str_subset('\\d{2}')
#spaces matter!
text %>% str_subset('\\d{2} ')
text %>% str_subset('^\\d')
```

## Other Resources

-   `RSelenium` for automating your web browser: [Package documentation](https://docs.ropensci.org/RSelenium/index.html); [Tutorial](http://joshuamccrain.com/tutorials/web_scraping_R_selenium.html)

-   [PML Intro to Python + Webscraping Workshop](https://github.com/adamkaplan0/PML_Web_scraping)

-   Quant IV Lectures/Recitations!

## Typical Workflow

1.  Get URLs needed to get target data
2.  Examine underlying html structure using *developer tools* in browser, with the help of SelectorGadget
3.  Create function that extracts relevant information from the page
4.  Test function on one/a few links to check that it generalizes
5.  Iterate over all URLs
6.  Clean data

## Scraping Int Security

-   Goal: Scrape title, abstract, author, and date of all IS articles published since 2022
-   What are the first steps?

## Step 1: Getting links

Let us first examine the website we are trying to scrape.

[Link](https://direct.mit.edu/isec/search-results?f_ContentType=Journal+Articles&fl_SiteID=1000045&rg_PublicationDate=01%2f01%2f2022+TO+02%2f20%2f2023&page=1)

## Step 1: Getting links

![Identifying relevant link](doi_link.png){fig-align="center"}

## Step 1: Getting links

```{r}
#| code-overflow: wrap
url <- "https://direct.mit.edu/isec/search-results?f_ContentType=Journal+Articles&fl_SiteID=1000045&rg_PublicationDate=01%2f01%2f2022+TO+02%2f20%2f2023&page=1"
html <- read_html(url)
html
```

## Step 1: Getting links

```{r}
html %>% html_elements(., xpath = '//a')
```

## Step 1: Getting links

```{r}
html %>% html_elements(., xpath = '//a') %>% html_attr('href')
```

## Step 1: Getting links

```{r}
#| code-fold: true
#| code-summary: "Show the code"

links <- html %>% html_elements('a') %>% html_attr('href') %>%
  str_subset(., 'doi')
head(links)
```

## Step 1: Getting links {.scrollable}

Now loop! But how to loop over different pages? Use the `glue` package

```{r}
text <- "Eyal and {name}"
name <- c('Ray', 'Gritty')
glue(text)
```

How would you use this to create a vector or search result links? (each element is one page of search results)

```{r}
#| code-fold: true
#| code-summary: "Show code"
# looping over pages 
out <- c()
#2 pages of results for 32 journal articles
url <- "https://direct.mit.edu/isec/search-results?f_ContentType=Journal+Articles&fl_SiteID=1000045&rg_PublicationDate=01%2f01%2f2022+TO+02%2f20%2f2023&page={no}"
for (i in 1:2){
  no <- i
  out <- c(out, glue(url))
}
out
```

## Step 1: Getting links

```{r}
#| label: get_links
#| code-overflow: wrap
# 2 pages, so using lapply is a bit silly
# but this code is generalizable 
links <- lapply(out, function(x){
  html <- read_html(x)
  # Sys.sleep(10) 
  links <- html %>% html_elements('a') %>% html_attr('href') %>%
    str_subset(., 'doi')
  return(links)
})
articles <- unlist(links)
```

## Step 2: Getting elements of interest

OK, now we have the links to each article. Let's inspect what an article [page](https://doi.org/10.1162/isec_a_00451) looks like.

![](screen_abstract.png)

## Step 2: Getting elements of interest

::: columns
::: {.column width="50%"}
How to get the abstract?

-   Need to identify what distinguishes the abstract chunk from other paragraph chunks
:::

::: {.column width="50%"}
![](abstract_code.png)
:::
:::

## Step 2: Getting elements of interest {.scrollable}

Using regex, extract the abstract from the html of the article. Hint: The workflow is

1.  Get the html of the article page using `read_html`
2.  Use what you learnt about extraction to abstract section using `html_elements`
3.  Use regex to extract the relevant chunk from the vector
4.  Convert to plaintext using `html_text`

```{r}
#| code-fold: true
#| code-summary: "Show code"
html_url <- articles[1]
html_article <- read_html(html_url) 
ab <- html_article %>% html_elements(css = 'section') %>% 
  .[grep('^<section class="abstract"><p>',.)] %>% html_text()
ab
```

## Step 2: Getting elements of interest {.scrollable}

Ditto for title, authors, and date.

```{r}
#title, using xpath to demontrate importance of using ' vs "
title <- html_article %>% html_elements(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "article-title-main", " " ))]') %>% 
  html_text()

#author
author <- html_article %>% html_elements(css = '.stats-author-info-trigger') %>% 
  html_text()
#collapsing 
author <- paste(author, collapse = ", ")

author
#date 
date <- html_article %>% html_elements(css = '.article-date') %>% 
  html_text %>% lubridate::mdy(.)
date
```

WAIT! Our title is a bit ugly

```{r}
title
```

Clean it up using `stringr` pacakge

```{r}
title %>% str_replace_all(., "[\r\n]" , "") %>% str_trim
```

## Step 2: Getting elements of interest {.scrollable}

```{r}
#| cache: true
#doing all
out_list <- lapply(1:length(articles), function(x){
  # Sys.sleep(30) #for longer tasks use sys.sleep

  html_article <- read_html(articles[x])
   
  #abstract
  abstract <- html_article %>% html_elements(css = 'section') %>% .[grep('^<section class="abstract"><p>',.)] %>% html_text()
  
  #title
  title <- html_article %>% html_elements(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "article-title-main", " " ))]') %>%
  html_text() %>% str_replace_all(., "[\r\n]" , "") %>% str_trim()
  
  #author
  author <- html_article %>% html_elements(css = '.stats-author-info-trigger') %>% 
    html_text()
  #collapsing 
  author <- paste(author, collapse = ", ")
  
  #date 
  date <- html_article %>% html_elements(css = '.article-date') %>% html_text %>% lubridate::mdy(.)
  
  list(t = title, au = author, d = date, ab = abstract)
})

df <- data.table::rbindlist(out_list, fill = T)
# write.xlsx(df, file = 'int_sec_art.xlsx')
```

## Step 3: Violà! {.scrollable}

```{r}
#| echo: false
library(gt)
gt::gt(df) %>% tab_options(
  table.font.size = px(20),
  container.overflow.x = TRUE,
  container.overflow.y = TRUE
) %>% as_raw_html()
```

## Scraping Press Releases {.scrollable}

-   Goal: \~10k releases from Pakistan's *Press Information Department (PID)* published between 2009 and 2011

-   ⚠️ Websites get taken down, links break, formatting changes

-   Two major sections: scraping data + cleaning data

![](releases.png){fig-align="center"}

## Goal:

![](releases_cloud.png){fig-align="center"}

## Step 1: Getting Links {.scrollable}

-   Let's take a look at the website we're going to scrape:

-   <https://web.archive.org/web/20111229043613/http://www.pid.gov.pk/pressarch2010-11.htm>

## Making a Plan {.scrollable}

1.  Collect link for each date (\~1000)
2.  Scrape the page for each date
3.  Split up the contents on the page into individual releases
4.  Data cleaning: make sure we're left with only press releases

-   If no time, download scraped data from Eyal's Github and then clean it: <https://github.com/ey-al/pml/blob/main/df_all.rData>

## Step 1: Collecting link for each date {.scrollable}

-   Start with the URL that has all 1000 dates, and make a vector of those links:

```{r}
#| code-fold: true
#| code-summary: "Show code"

#Link we're starting from (looks like a calendar of dates)
startingURL <- "https://web.archive.org/web/20111229043613/http://www.pid.gov.pk/pressarch2010-11.htm"

#Read the HTML of that calendar page
get_individuallinks <- read_html(startingURL) %>% 
  
  #Pull all elements with a href (aka links!)
  html_elements('a') %>% 
  html_attr("href")
```

## ⛔️ Before You Start:

-   Not every link is going to work! (some pages not captured by archive.org)
-   So, use `{r} tryCatch()` to try each page and return an error if failure
-   Otherwise, scraper will break randomly in the middle of the process (or the middle of the night!)

## Step 2: Scrape the page for each date {.scrollable}

-   **End Goal**: Data frame with 2 columns, *pr_text* and *pr_date*.

-   Text is the text scraped from the page, date is just the link we're scraping (the date is in the link 🙃)

-   Follows the same format as the *IS* example

```{r}
#| echo: false
load("df_all.rData")
```

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| code-overflow: wrap
#| eval: FALSE

#Start with a lapply over the 1005 links you collected
pr_out_list <- lapply(1:1005, function(x){
  
  #Might be a good idea to take a 30 second break between each call
  #Sys.sleep(30) 
  
  #Part 1: collecting text
  text <- tryCatch(
    {
    #R console is going to spit this out each time it tries to scrape  
    message("This is the 'try' part")
    
    #The URL we are scraping = the base archive.org/pid link + the date link we scraped already    
    read_html(paste0("https://web.archive.org/web/20130502224925/http://pid.gov.pk/", get_individuallinks[x])) %>% 
      
    #Collecting the HTML tables on the page  
    html_elements("td") %>%
    #Collecting the text from those tables
    html_text() %>% 
    #Trimming whitespace  
    trimws() %>%
    #Pasting everything back together (bunch of tables)  
    paste(., collapse = " ")
    },
    
    #R console is going to spit this out each time it fails 
    error=function(cond) {
      message(paste("URL does not seem to exist:", get_individuallinks[x]))
      message("Here's the original error message:")
      message(cond)
      return(NA)
    }
  )
  
  #Part 2: collecting the date (just getting the url of we scraped)
  date <- get_individuallinks[x] 
  
  #Putting the text and URL together
  list(pr_text = text, pr_date = date)
})

#Combining the list into a df
df_all <- data.table::rbindlist(pr_out_list, fill = T)
#save(df_all, file= "df_all.rData")
```

## HINTS {.scrollable}

-   Remove rows/dates that are *NA* / we weren't able to scrape
-   Convert all text to uppercase
-   Lots of random line breaks/whitspace can be removed with `"[\r\n]"`
-   Use `strsplit` and `unnest` to split up the chunks of text at a meaningful spot
-   After the splitting, use `str_detect` and `nchar` to remove chunks that don't have anything useful. Example: "PR 146" or "(NOT TO BE PUBLISHED, BROADCAST, AND TELECAST BEFORE THE MORNING OF JANUARY 1, 2012)"
-   Get the pr_date into something that `lubridate` will understand (day-month-year)

## Steps 3 and 4: Split up releases and clean {.scrollable}

Using tidyverse to put everything in one code chunk, but steps can also be split up!

```{r}
#| code-fold: true
#| code-summary: "Show code"

#Take the result of the previous scrape (or the downloaded input from Eyal)
releases <- df_all %>% 
  
  #Remove rows that have NA for pr_text
  drop_na(pr_text) %>%
  
  #Split pr_text strings at "PRESS RELEASE" (after converting it all to uppercase)
  mutate(pr_text = strsplit(as.character(toupper(pr_text)), "PRESS RELEASE")) %>% 
  
  #Unnest these split up strings (from 1 row to ~5 or 6 rows per date)
  unnest(pr_text) %>%
  
  #Remove line breaks from pr_text
  mutate(pr_text = str_remove_all(pr_text, "[\r\n]")) %>% 
  
  #Remove rows that have "not to be published" text, BUT only those with less than 200 characters
  filter(!(str_detect(pr_text, "NOT TO BE PUBLISHED, BROADCAST") & nchar(as.character(pr_text))<=200)) %>%
  
  #Remove other entries that have less than 200 characters
  filter(nchar(pr_text)>=200) %>%
  
  #Remove the terms press, .htm, and issue from the date column
  mutate(pr_date = str_remove_all(pr_date, "press|.htm|issue")) %>%
  
  #Use lubridate to get the pr_date column into a date format
  mutate(pr_date = dmy(pr_date))
```

## Part 1: Visualize the data with a bar graph {.scrollable}

```{r}
#| code-fold: true
#| code-summary: "Show code"

#Do a count of PRs by year-month and make a bar graph
ggplot(releases, aes(format(pr_date, "%Y-%m"))) +
  geom_bar(stat = "count") +
  theme(axis.text.x=element_text(angle=60, hjust=1))+
  labs(title="Scraped Press Releases, Pakistan Press Information Department" ,x = "Month", 
       y = "Number of Releases")

```

## Part 2: Visualize the data with a word cloud {.scrollable}

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| eval: FALSE
library(quanteda)
library("quanteda.textplots")

#Convert the df to a quanteda corpus
pr_corpus <- corpus(releases, text_field = "pr_text")

#Create a Document-Feature Matrix (DFM) of the PR terms — but remove common stopwords ("for," "and," "if," etc..), punctuation, symbols, and numbers
pr_dfm <- dfm(pr_corpus, remove = stopwords("english"),
              remove_punct = TRUE,
              remove_symbols = TRUE,
              remove_numbers = TRUE)

#Make a wordcloud!
textplot_wordcloud(pr_dfm, min_count = 6, max_words = 100)

```

![](releases_cloud.png){fig-align="center"}

## Thank you!

-   Please scrape responsibly!
    -   Add delays between pages
    -   Use distributed scraping only against rich sites
